# RCA Report: AKS Vault Tier Tiering Failure

**Task ID:** `de303553-8313-4719-a9f5-0e6159b20599`  
**Date:** 2026-01-09 05:03:05 - 22:36:37 UTC  
**Duration:** 17h 33m 32s  
**Status:** CloudInternalError  

---

## Summary

AKS Vault Tier tiering job for backup instance `aks-Estaid-WEU-Prod` failed with `CloudInternalError` because the DataMover FullIC (Full Integrity Check) operation for storage unit **`monitoring_storage-loki-0`** crashed with a `System.NullReferenceException` in the in-memory queue, causing the job to hang at 0% progress until the 720-minute timeout was exceeded and the workflow was aborted.

---

## Root Cause

A **DataMover service bug** caused a `System.NullReferenceException` in `InmemorySafeQueue.cs` (lines 60/68 for Enqueue, lines 109/116 for Dequeue) within the `ProducerConsumerEngine` during the FullIC `ValidateData` phase for DM operation `98866d6f-1aaa-4a08-aa50-d192f5b9f2f6`. This crashed both the producer and consumer threads of the FullIC engine (`98866d6f-1aaa-4a08-aa50-d192f5b9f2f6_FullIcEngine`), leaving the job stuck in `ValidateData/InProgress` state indefinitely.

**Exception:**
```
System.NullReferenceException: Object reference not set to an instance of an object.
   at InmemorySafeQueue`1.Enqueue(T buffer) in InmemorySafeQueue.cs:line 68
   at ProducerConsumerEngine`2.ProducerInternal() in ProducerConsumerEngine.cs:line 299
```

---

## Evidence

| Field | Value |
|-------|-------|
| **Vault** | `bv-Estaid-WEU-Prod-Public` |
| **AKS Cluster** | `aks-Estaid-WEU-Prod` |
| **Region** | `westeurope` |
| **Deployment** | `we-pod01` |
| **DM Node** | `we-pod01-dm-01-sf` (DMWorker_2409) |
| **Subscription** | `14131137-7139-466e-a154-9fc87edd99d2` |
| **Backup Instance ID** | `262cd5ea-ef8d-4f33-9130-f40e21ecb0c9` |
| **Job ID** | `5ef1f927-bbb2-4fb5-956c-03cd975c632a` |
| **Recovery Point ID** | `afe9d5fbc2c84f84ab60710dd79882fa` |
| **Policy** | `Azure-Kubernetes` |

### Storage Units Overview

| Type | Count | Names |
|------|-------|-------|
| KubernetesCSIAzureDiskSnapshot | 10 | cosmo-prod_estaid-file-pvc-v2, cosmo_cosmo-minio, cosmo_data-cosmo-clickhouse-shard0-0, cosmo_data-cosmo-clickhouse-shard0-1, cosmo_redis-data-cosmo-redis-master-0, cosmo_redis-data-cosmo-redis-replicas-0, default_verdaccio-verdaccio, monitoring_prometheus-stack-grafana, **monitoring_storage-loki-0**, production_estaid-file-pvc-v2 |
| KubernetesClusterStateSnapshot | 1 | clusterstatestorageunit |

### Workflow Timeline

| Time (UTC) | Event |
|------------|-------|
| 05:03:05 | Task created, PreTaskHandler → InitializeTiering → QueryRetentionPointTask |
| 05:03:05 | TieringInDataMoverHelperTask started (Tiering phase) |
| 05:04:54 | Tiering DM operations submitted |
| 05:19:24 | Tiering phase completed (all SUs submitted successfully) |
| ~10:23:22 | RP committed with `PendingFullIC` state (32 storage units) |
| 10:27:52 | FullIC DM operation `98866d6f-1aaa-4a08-aa50-d192f5b9f2f6` tracking started |
| 11:14:57 | DM job entered `ExternalQAssignment` → Succeeded (0.01 min) |
| ~11:15 | DM job entered `ValidateData` state |
| **17:37:53** | **NullReferenceException in InmemorySafeQueue - FullIcEngine crashed** |
| 17:37:53-56 | Multiple NullReferenceExceptions in both Producer and Consumer threads |
| 17:37 - 22:25 | Job stuck at 0% progress; RAS polling every 3 minutes |
| 22:25:34 | IntegrityCheckInDataMoverHelperTask aborted (exceeded 720-min timeout) |
| 22:34:25 | Cancel request sent to DM; DM returns `JobNotFound` (job already dead) |
| 22:36:37 | EndTieringTask → PostTaskHandler → Job marked as `CloudInternalError` |

### DM Operation States

| Operation ID | State | Status | Duration (min) |
|-------------|-------|--------|---------------|
| `98866d6f-...` | ExternalQAssignment | Succeeded | 0.01 |
| `98866d6f-...` | ValidateData | **InProgress (stuck)** | **255.44+** |

### All Other FullIC Operations (9/10 succeeded)

| Storage Unit | DM Operation ID | Stage | Status |
|---|---|---|---|
| cosmo-prod_estaid-file-pvc-v2 | `3d540eaf-3013-47c1-9335-f2db1fcb2ba7` | FullICCommitRP | Succeeded |
| cosmo_cosmo-minio | `2faba659-7bea-4d55-98ad-277cddc65b74` | FullICCommitRP | Succeeded |
| cosmo_data-cosmo-clickhouse-shard0-0 | `de657ec5-c3b8-4f89-85a8-43dfe3149173` | FullICCommitRP | Succeeded |
| cosmo_data-cosmo-clickhouse-shard0-1 | `4ba06cff-0616-4b42-b32c-4e538d109b02` | FullICCommitRP | Succeeded |
| cosmo_redis-data-cosmo-redis-master-0 | `80e96f39-a3fd-46d0-ad9c-7bf3c60c54e3` | FullICCommitRP | Succeeded |
| cosmo_redis-data-cosmo-redis-replicas-0 | `1ce3420b-5334-46ce-9ddc-a95ea04ba9c7` | FullICCommitRP | Succeeded |
| default_verdaccio-verdaccio | `83d6584c-b60e-434e-93cc-908cb0221725` | FullICCommitRP | Succeeded |
| monitoring_prometheus-stack-grafana | `ff521690-28ec-4a91-9e92-fc7d960387d2` | FullICCommitRP | Succeeded |
| production_estaid-file-pvc-v2 | `ec42b5bd-a8fe-4980-8ac5-808287ea4b59` | FullICCommitRP | Succeeded |
| **monitoring_storage-loki-0** | **`98866d6f-1aaa-4a08-aa50-d192f5b9f2f6`** | **TriggerFullIC** | **FAILED** |

---

## Recommendation

1. **Retry the tiering operation** - This is a transient DataMover service bug (NullReferenceException in the in-memory queue). A retry should succeed since 9/10 disk SUs have already completed FullIC and only `monitoring_storage-loki-0` needs to be re-processed.

2. **Escalate to DataMover team** - File a bug against the DataMover service for the `NullReferenceException` in `InmemorySafeQueue.cs` (lines 60-68, 109-116) within `ProducerConsumerEngine.cs`. The DM version is `1.0.160.0-master` running on `we-pod01-dm-01-sf`. The bug manifests during the `ValidateData` phase of FullIC operations and causes the ProducerConsumerEngine's producer and consumer threads to both crash silently without failing the job, leaving it stuck forever.

3. **Monitor `monitoring_storage-loki-0` disk size** - This is a Loki log storage PVC which can grow large. If the disk is very large, the FullIC validation may need longer timeout or better handling in the DataMover service.

4. **RAS version:** `1.0.237.0-master`, **DM version:** `1.0.160.0-master`
